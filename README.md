# Tennis Cut

This repository contains tooling for extracting tennis swing clips from raw match footage. The system uses an audio "pop" detector to find candidate impact moments and then verifies them with a lightweight vision model. Full design details are available in [spec.md](spec.md).

To process a video do this:
1. Shoot in slow motion on an iPhone, you can use 4k/1080/240fps/120fps (120fps/4k is recommended)
2. Export the raw version on your iPhone to files (Necessary because if you just try to export it, apple mixes the different frame rates)
3. Airdrop or send that raw file to your computer, put the file in `to_process_vids`
4. Run the following command from the root of this directory: `uv sync && uv run tennis-cut -v to_process_vids`
(For timing, for 1080p it takes around 2 sec per a hit, and for 4k, about 7 sec per a hit)

## Directory overview

- `videos/` – this has all the training videos for the project, too keep the file size down I
generally keep these as 120 FPS, 1080p files so they are smaller, but they don't have to be. If I have a new video to label
see the README in label_videos.
- `dataset/` - this has the extracted image files from videos in `videos`. This is generated by running
`TODO`
- `wavs/` - similar to `dataset` this has the extracted `wav` files from the videos in `videos`, this
is extracted by running `TODO`

- `src/` – application source code and scripts; subpackages: (See the README files inside `src/` subfolders too)
	- `label_videos/` – GUI and scripts to annotate impacts (video prep + annotation tools).
	- `train_pop_detector/` – prepare 0.25s audio windows and train the audio "pop" model.
	- `train_swing_detector/` – extract labeled frames and train the swing image classifier.
	- `tennis_cut/` – CLI for detecting swings in videos and cutting/stitching clips.
	- `utilities/` – shared helpers (ffprobe duration, YOLO person box, cropping).

- `test_videos/` – (Currently unused, but meant for testing models)
- `to_process_vids/` - Videos that need to be processed/cut up
- `tmp_videos/` – This is just an extra folder for files that are halfway done, I shouldn't leave things in here
- `models/` – trained models, both the pop (audio) classifier and the swing (image) classifier

- `examples/` – mini example media that is supposed to be used by LLMs to test out scripts
- `meta/` – CSV metadata used during training


## Labeling videos

If I want to label more videos follow this pipline:

1. Label the new video (see below)
2. After doing this, regenerate the audio model (optional)
3. Also regenerate the swing model (optional)


## Linting

The project uses [Ruff](https://docs.astral.sh/ruff/) for linting. To check the
code locally run:

```bash
uv sync
source .venv/bin/activate
ruff check .
```

Ruff runs automatically on pull requests to the `main` branch via GitHub
Actions.


## TODO

- [ ] Update the specs
- [ ] Make sure the examples work correctly
- [ ] Figure out a way to click one button on my phone and it will copy the files to my local server and run the pipeline (and upload them somewhere?)
- [ ] Develop test to see if the the models are getting better (probably some test set and a score on that test set)
- [ ] In tennis_annotate.py part have the key part have a fixed height and the image fill the rest
- [ ] In tennis_annotage.py don't let you have labels that are closer than 0.5 seconds apart
- [ ] Revert last hit in tennis_annotate.py
- [ ] Display the last 3 hits in tennis_annotate.py in the UI
