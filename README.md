# Tennis Cut

This repository contains tooling for extracting tennis swing clips from raw match footage. The system uses an audio "pop" detector to find candidate impact moments and then verifies them with a lightweight vision model. Full design details are available in [spec.md](spec.md).

To process a video do this:
1. Shoot in slow motion on an iPhone, you can use 4k/1080/240fps/120fps (Need to figure out a recommendation)
2. Export the raw version on your iPhone to files (Necessary because if you just try to export it, apple mixes the different frame rates)
3. Airdrop or send that raw file to your computer
4. Run the following command: `TODO`

## Directory overview

- `videos/` – this has all the training videos for the project, too keep the file size down I
generally keep these as 120 FPS, 1080p files, but they don't have to be. If there is a video
file that is missing a label, then you can run ..... to manually tag a video.
- `dataset/` - this has the extracted image files from videos in `videos`. This is generated by running
`TODO`
- `wavs/` - similar to `dataset` this has the extracted `wav` files from the videos in `videos`, this
is extracted by running `TODO`

- `src/` – application source code and scripts; subpackages: (See the README files inside `src/` subfolders too)
	- `label_videos/` – GUI and scripts to annotate impacts (video prep + annotation tools).
	- `train_pop_detector/` – prepare 0.25s audio windows and train the audio "pop" model.
	- `train_swing_detector/` – extract labeled frames and train the swing image classifier.
	- `tennis_cut/` – CLI for detecting swings in videos and cutting/stitching clips.
	- `utilities/` – shared helpers (ffprobe duration, YOLO person box, cropping).

- `test_videos/` – small clips used for unit-level manual tests and quick validation.
- `tmp_videos/` – This is just an extra folder for files that are halfway done, I shouldn't leave things in here
- `models/` – trained models, both the pop (audio) classifier and the swing (image) classifier

- `examples/` – mini example media that is supposed to be used by LLMs to test out scripts
- `meta/` – CSV metadata used during training


## Linting

The project uses [Ruff](https://docs.astral.sh/ruff/) for linting. To check the
code locally run:

```bash
pip install -e .
ruff .
```

Ruff runs automatically on pull requests to the `main` branch via GitHub
Actions.

## Contributing

See AGENTS.md for contributor guidelines, project structure, and development commands. Please run `ruff .` locally and include brief testing steps in your PR descriptions.


## TODO

Below is a short example checklist you can use as a starting point for common tasks.

- [ ] Download example media into `examples/videos` and `examples/wavs`:

```bash
wget 'https://www.dropbox.com/scl/fi/dce0wabuy0kss3xtcp7th/tester.MOV?rlkey=y8cwf7wssvswq1dxj12rxrrhw&st=i4yi7aeh&dl=0' -O examples/videos/tester.MOV
wget 'https://www.dropbox.com/scl/fi/5pxrm1y9ij8qvls07ve03/tester.wav?rlkey=rxlfhsdxigrqf3zwye6jk8b1q&st=icvmew0l&dl=0' -O examples/wavs/tester.wav
```

- [ ] Prepare audio windows for training:

```bash
python src/train_pop_detector/prepare_audio_windows.py --videos_dir examples/videos --wav_dir examples/wavs --out_csv examples/meta/labled_windows.csv
```

- [ ] Train a quick smoke-run of the audio pop detector (1 epoch, CPU):

```bash
python src/train_pop_detector/train_audio_pop.py examples/meta/labled_windows.csv --epochs 1 --device cpu --bs 8
```

- [ ] Run the cutter on example media to produce clips:

```bash
python src/tennis_cut/tennis_cut.py examples/videos --model models/<audio_model>.pth --no-stitch
```

